{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping PSAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from psaw import PushshiftAPI\n",
    "import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep_posts(subreddit, start_time, end_time,  limit):\n",
    "    api = PushshiftAPI()\n",
    "    filters = ['id', 'author', 'created_utc',\n",
    "                'domain', 'url',\n",
    "                'title', 'num_comments','selftext']                 \n",
    "                #We set by default some useful columns\n",
    "\n",
    "    posts = list(api.search_submissions(\n",
    "        subreddit=subreddit,   #Subreddit we want to audit\n",
    "        after=start_time,      #Start date\n",
    "        before=end_time,       #End date\n",
    "        filter=filters,        #Column names we want to retrieve\n",
    "        limit=limit))          ##Max number of posts\n",
    "\n",
    "    # df = pd.DataFrame(posts)\n",
    "\n",
    "    return pd.DataFrame([thing.d_ for thing in posts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = int(dt.datetime(year=2018, month=1, day=1).timestamp()) \n",
    "end_time = int(dt.datetime(year=2018, month=1, day=2).timestamp()) \n",
    "\n",
    "df=data_prep_posts('wallstreetbets',start_time,end_time,None)\n",
    "\n",
    "# start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(days=365)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = int(dt.datetime(year=2018, month=1, day=1).timestamp()) \n",
    "end_time = int(dt.datetime(year=2018, month=1, day=2).timestamp()) \n",
    "\n",
    "start_time-end_time\n",
    "\n",
    "dt.datetime(year=2018, month=1, day=1)-dt.datetime(year=2017, month=1, day=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dataset(start,end,subreddit,limit):\n",
    "#     pd.DataFrame().to_csv(\"../data/raw/\" + subreddit + \".csv\", index=False, header=False)\n",
    "\n",
    "#     delta = end - start\n",
    "\n",
    "#     for d in range(delta.days + 1):\n",
    "#         d_1=dt.timedelta(days=1)\n",
    "#         d_n=dt.timedelta(days=d+1)\n",
    "#         start_get=int((start+d_n-d_1).timestamp())\n",
    "#         end_get=int((start+d_n).timestamp())\n",
    "        \n",
    "#         df=data_prep_posts(subreddit,start_get,end_get,limit)\n",
    "#         df.to_csv(\"../data/raw/\" + subreddit + \".csv\", mode='a', index=False, header=False)\n",
    "#         print(d)\n",
    "        \n",
    "# start=dt.datetime(year=2018, month=1, day=1)\n",
    "# end=dt.datetime(year=2018, month=1, day=5)\n",
    "\n",
    "# create_dataset(start,end,'wallstreetbets',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['author', 'created_utc', 'domain', 'id', 'num_comments', 'selftext',\n",
    "       'title', 'url', 'created', 'd_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'created_utc', 'domain', 'id', 'num_comments', 'selftext',\n",
       "       'title', 'url', 'created', 'd_'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../data/raw/wallstreetbets.csv').keys"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4efcd80d53f331e60d4420b3949daf20df6e0af725842628a26694ea28d8e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
