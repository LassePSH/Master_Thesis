{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining, AdamW, get_scheduler, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ElectraModel\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error() #Remove warning msg - missing fine-tunning\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "  def __init__(self, texts, targets, tokenizer, max_len,network_features):\n",
    "    self.network_features = network_features\n",
    "    self.text = texts\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    network_features = self.network_features[item]\n",
    "    text = str(self.text[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return {\n",
    "        'network_features': torch.tensor(network_features, dtype=torch.float),\n",
    "        'text': text,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'targets': torch.tensor(target, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/pelle/Master_Thesis/data/processed/dataloaders/week10/'\n",
    "\n",
    "eval_dataloader = torch.load(path+'eval_dataloader_full.pt')\n",
    "train_dataloader = torch.load(path+'train_dataloader_full.pt')\n",
    "test_dataloader = torch.load(path+'test_dataloader_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dict_keys(['network_features', 'text', 'input_ids', 'attention_mask', 'targets'])\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dict_keys(['network_features', 'text', 'input_ids', 'attention_mask', 'targets'])\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dict_keys(['network_features', 'text', 'input_ids', 'attention_mask', 'targets'])\n"
     ]
    }
   ],
   "source": [
    "print(type(eval_dataloader))\n",
    "print(iter(eval_dataloader).next().keys())\n",
    "\n",
    "print(type(train_dataloader))\n",
    "print(iter(train_dataloader).next().keys())\n",
    "\n",
    "print(type(test_dataloader))\n",
    "print(iter(test_dataloader).next().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696\n",
      "45408\n",
      "5696\n",
      "\n",
      "Sum\n",
      "56800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 32\n",
    "print(len(eval_dataloader)*batch)\n",
    "print(len(train_dataloader)*batch)\n",
    "print(len(test_dataloader)*batch)\n",
    "print()\n",
    "# sum\n",
    "print('Sum')\n",
    "print(len(eval_dataloader)*batch + len(train_dataloader)*batch + len(test_dataloader)*batch)\n",
    "\n",
    "\n",
    "# get batch size\n",
    "next(iter(eval_dataloader))['targets'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "electraModel = ElectraModel.from_pretrained('google/electra-small-discriminator')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "dataiter = iter(eval_dataloader)\n",
    "n_features = next(dataiter)['network_features'].shape[1]\n",
    "print(n_features)\n",
    "\n",
    "class ElectraClassifier(nn.Module):\n",
    "    def __init__(self,num_labels=2):\n",
    "        super(ElectraClassifier,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # network features\n",
    "        self.network_input = nn.Linear(in_features=9,out_features=2048) # 9 network features\n",
    "        self.dense_net2 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net3 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net4 = nn.Linear(in_features=2048,out_features=2048)\n",
    "\n",
    "        # output layer\n",
    "        self.out_proj = nn.Linear(2048, self.num_labels)\n",
    "\n",
    "\n",
    "    def forward(self,network_features=None):\n",
    "        x_net = self.network_input(network_features)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net2(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net3(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net4(x_net)\n",
    "        x = F.gelu(x_net)\n",
    "\n",
    "        logits = self.out_proj(x_net)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model=ElectraClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1563, 0.1171]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5098, 0.4902]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "iter(test_dataloader).next()['targets']\n",
    "\n",
    "# random torch tensor\n",
    "network_features = torch.rand(1, 524)\n",
    "\n",
    "dense=nn.Linear(in_features=524,out_features=2)\n",
    "\n",
    "sm=nn.Softmax(dim=1)\n",
    "\n",
    "# torch.max(sm(dense(network_features)),dim=1)\n",
    "print(dense(network_features))\n",
    "print(sm(dense(network_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False,no_deprecation_warning=True)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      network_features = d[\"network_features\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        network_features=network_features)\n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    network_features = d[\"network_features\"].to(device)\n",
    "    \n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      network_features=network_features)\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = defaultdict(list)\n",
    "# best_accuracy = 0\n",
    "# for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "#   train_acc, train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device, scheduler,len(train_dataloader.dataset))\n",
    "#   val_acc, val_loss = eval_model(model, eval_dataloader, loss_fn, device, len(eval_dataloader.dataset))\n",
    "\n",
    "#   history['train_acc'].append(train_acc)\n",
    "#   history['train_loss'].append(train_loss)\n",
    "#   history['val_acc'].append(val_acc)\n",
    "#   history['val_loss'].append(val_loss)\n",
    "  \n",
    "#   if val_acc > best_accuracy:\n",
    "#     torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "#     best_accuracy = val_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "electraModel = ElectraModel.from_pretrained('google/electra-small-discriminator')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "dataiter = iter(eval_dataloader)\n",
    "n_features = next(dataiter)['network_features'].shape[1]\n",
    "print(n_features)\n",
    "\n",
    "class ElectraClassifier(nn.Module):\n",
    "    def __init__(self,num_labels=2):\n",
    "        super(ElectraClassifier,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # network features\n",
    "        self.network_input = nn.Linear(in_features=9,out_features=2048) # 9 network features\n",
    "        self.dense_net2 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net3 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net4 = nn.Linear(in_features=2048,out_features=2048)\n",
    "\n",
    "        # output layer\n",
    "        self.out_proj = nn.Linear(2048, 2)\n",
    "\n",
    "\n",
    "    def forward(self,network_features=None):\n",
    "        x_net = self.network_input(network_features)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net2(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net3(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net4(x_net)\n",
    "        x = F.gelu(x_net)\n",
    "\n",
    "        logits = self.out_proj(x_net)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model=ElectraClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 9\n",
    "hidden_sizes = [2048, 64]\n",
    "output_size = 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([[0.9577, 0.0423],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.6927, 0.3073],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.8944, 0.1056],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.6693, 0.3307],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    d = next(iter(test_dataloader))\n",
    "    network_features = d[\"network_features\"].to(device)\n",
    "    logits = model(network_features)\n",
    "    _, preds = torch.max(logits, dim=1)\n",
    "    print(preds)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([[0.9577, 0.0423],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.6927, 0.3073],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.8944, 0.1056],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.6693, 0.3307],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000]])\n",
      "tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    ground_truth = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            network_features = d[\"network_features\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(network_features)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            ground_truth.extend(targets)\n",
    "            break\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    ground_truth = torch.stack(ground_truth).cpu()\n",
    "\n",
    "    return predictions, prediction_probs, ground_truth\n",
    "\n",
    "y_pred, y_pred_probs, y_test = get_predictions(model,test_dataloader)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_pred_probs)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ElectraModel\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ElectraClassifier(nn.Module):\n",
    "    def __init__(self,num_labels=2):\n",
    "        super(ElectraClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # text features\n",
    "        self.electra = ElectraModel.from_pretrained('google/electra-small-discriminator')\n",
    "        self.dense_txt = nn.Linear(self.electra.config.hidden_size, self.electra.config.hidden_size) # 256\n",
    "        self.dropout_txt = nn.Dropout(self.electra.config.hidden_dropout_prob)\n",
    "\n",
    "        # combined features\n",
    "        self.dense_cat1 = nn.Linear(in_features=(256+12),out_features=512) # 256 from text features + 12 from network features \n",
    "        self.dense_cat2 = nn.Linear(in_features=512,out_features=1024)\n",
    "        self.dense_cat3 = nn.Linear(in_features=1024,out_features=2048)\n",
    "        self.dense_cat4 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_cat5 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_cat6 = nn.Linear(in_features=2048,out_features=1024)\n",
    "        self.dense_cat7 = nn.Linear(in_features=1024,out_features=512)\n",
    "        self.dense_cat8 = nn.Linear(in_features=512,out_features=256)\n",
    "\n",
    "        # output layer\n",
    "        self.out_proj = nn.Linear(256, self.num_labels) # 2 labels\n",
    "\n",
    "    def classifier(self,sequence_output,network_features):\n",
    "        # text features\n",
    "        x_txt = sequence_output[:, 0, :] #[CLS] token\n",
    "        x_txt = F.relu(self.dense_txt(x_txt))\n",
    "        x_txt = self.dropout_txt(x_txt)\n",
    "        \n",
    "        # combined features\n",
    "        x = torch.cat((x_txt,network_features),dim=1) \n",
    "        x = F.relu(self.dense_cat1(x))\n",
    "        x = F.relu(self.dense_cat2(x))\n",
    "        x = F.relu(self.dense_cat3(x))\n",
    "        x = F.relu(self.dense_cat4(x))\n",
    "        x = F.relu(self.dense_cat5(x))\n",
    "        x = F.relu(self.dense_cat6(x))\n",
    "        x = F.relu(self.dense_cat7(x))\n",
    "        x = F.relu(self.dense_cat8(x))\n",
    "\n",
    "        # output layer\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids=None,attention_mask=None,network_features=None):\n",
    "        discriminator_hidden_states = self.electra(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "        logits = self.classifier(sequence_output,network_features)\n",
    "        # F.softmax(logits,dim=1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = ElectraClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Dataset class\n",
    "class Dataset():\n",
    "  def __init__(self, texts, targets, tokenizer, max_len,network_features):\n",
    "    self.network_features = network_features\n",
    "    self.text = texts\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    network_features = self.network_features[item]\n",
    "    text = str(self.text[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return {\n",
    "        'network_features': torch.tensor(network_features, dtype=torch.float),\n",
    "        'text': text,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'targets': torch.tensor(target, dtype=torch.long)}\n",
    "\n",
    "def create_dataloader(df, tokenizer, max_len, batch_size):\n",
    "\n",
    "    ds = Dataset(\n",
    "    network_features=df[['degree_cen', 'close_cen', 'activity', 'degree', 'N_nodes', 'N_edges','mentions',\n",
    "                        'frac_rec','N_rec','degree_in','degree_out','N_rec_author']].to_numpy(),\n",
    "    texts=df[\"text_title\"].to_numpy(),\n",
    "    targets=df['awarded'].to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len)\n",
    "\n",
    "    return DataLoader(ds,batch_size=batch_size,num_workers=2)\n",
    "\n",
    "df_eval = pd.read_csv(\"/home/pelle/Master_Thesis/data/processed/splitted/eval.csv\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/electra-small-discriminator')\n",
    "\n",
    "dl = create_dataloader(df_eval,tokenizer,200,32)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device: ' + str(device))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for d in dl:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        network_features = d[\"network_features\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        network_features=network_features)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dot: graph is too large for cairo-renderer bitmaps. Scaling by 0.954889 to fit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'electra_torchviz.png'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# make_dot(outputs.mean(), params=dict(list(model.named_parameters()))).render(\"rnn_torchviz\", format=\"png\")\n",
    "make_dot(model(input_ids=input_ids,attention_mask=attention_mask,network_features=network_features).mean(), params=dict(list(model.named_parameters()))).render(\"electra_torchviz\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parameters in electra\n",
    "sum(p.numel() for p in model.electra.parameters() if p.requires_grad)\n",
    "\n",
    "# number of parameters in dense_cat layers\n",
    "sum(p.numel() for p in model.dense_cat1.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat2.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat3.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat4.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat5.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat6.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat7.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat8.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "sum(p.numel() for p in model.out_proj.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ElectraModel\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ElectraClassifier(nn.Module):\n",
    "    def __init__(self,num_labels=2):\n",
    "        super(ElectraClassifier, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # text features\n",
    "        self.electra = ElectraModel.from_pretrained('google/electra-small-discriminator')\n",
    "        self.dense_txt = nn.Linear(self.electra.config.hidden_size, self.electra.config.hidden_size) # 256\n",
    "        self.dropout_txt = nn.Dropout(self.electra.config.hidden_dropout_prob)\n",
    "\n",
    "        # combined features\n",
    "        self.dense_cat1 = nn.Linear(in_features=(256+12),out_features=512) # 256 from text features + 12 from network features \n",
    "        self.dense_cat2 = nn.Linear(in_features=512,out_features=1024)\n",
    "        self.dense_cat3 = nn.Linear(in_features=1024,out_features=2048)\n",
    "        self.dense_cat6 = nn.Linear(in_features=2048,out_features=1024)\n",
    "        self.dense_cat7 = nn.Linear(in_features=1024,out_features=512)\n",
    "        self.dense_cat8 = nn.Linear(in_features=512,out_features=256)\n",
    "\n",
    "        # output layer\n",
    "        self.out_proj = nn.Linear(256, self.num_labels) # 2 labels\n",
    "\n",
    "    def classifier(self,sequence_output,network_features):\n",
    "        # text features\n",
    "        x_txt = sequence_output[:, 0, :] #[CLS] token\n",
    "        x_txt = F.relu(self.dense_txt(x_txt))\n",
    "        x_txt = self.dropout_txt(x_txt)\n",
    "        \n",
    "        # combined features\n",
    "        x = torch.cat((x_txt,network_features),dim=1) \n",
    "        x = F.relu(self.dense_cat1(x))\n",
    "        x = F.relu(self.dense_cat2(x))\n",
    "        x = F.relu(self.dense_cat3(x))\n",
    "        x = F.relu(self.dense_cat4(x))\n",
    "        x = F.relu(self.dense_cat5(x))\n",
    "        x = F.relu(self.dense_cat6(x))\n",
    "        x = F.relu(self.dense_cat7(x))\n",
    "        x = F.relu(self.dense_cat8(x))\n",
    "\n",
    "        # output layer\n",
    "        logits = self.out_proj(x)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids=None,attention_mask=None,network_features=None):\n",
    "        discriminator_hidden_states = self.electra(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = discriminator_hidden_states[0]\n",
    "        logits = self.classifier(sequence_output,network_features)\n",
    "        # F.softmax(logits,dim=1)\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = ElectraClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-small-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5516544"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ElectraClassifier()\n",
    "\n",
    "# # number of parameters in dense_cat layers\n",
    "sum(p.numel() for p in model.dense_cat1.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat2.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat3.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat6.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat7.parameters() if p.requires_grad) + \\\n",
    "sum(p.numel() for p in model.dense_cat8.parameters() if p.requires_grad)\n",
    "\n",
    "# model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1f814a4e109c882a9affb42993ee939858e43ebdc57bfb1498f07e0b82aab6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
