{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn, optim\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining, AdamW, get_scheduler, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ElectraModel\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error() #Remove warning msg - missing fine-tunning\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "  def __init__(self, texts, targets, tokenizer, max_len,network_features):\n",
    "    self.network_features = network_features\n",
    "    self.text = texts\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    network_features = self.network_features[item]\n",
    "    text = str(self.text[item])\n",
    "    target = self.targets[item]\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      text,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "    return {\n",
    "        'network_features': torch.tensor(network_features, dtype=torch.float),\n",
    "        'text': text,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'targets': torch.tensor(target, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/pelle/Master_Thesis/data/processed/dataloaders/week10/'\n",
    "\n",
    "eval_dataloader = torch.load(path+'eval_dataloader_full.pt')\n",
    "train_dataloader = torch.load(path+'train_dataloader_full.pt')\n",
    "test_dataloader = torch.load(path+'test_dataloader_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dict_keys(['network_features', 'text', 'input_ids', 'attention_mask', 'targets'])\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dict_keys(['network_features', 'text', 'input_ids', 'attention_mask', 'targets'])\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "dict_keys(['network_features', 'text', 'input_ids', 'attention_mask', 'targets'])\n"
     ]
    }
   ],
   "source": [
    "print(type(eval_dataloader))\n",
    "print(iter(eval_dataloader).next().keys())\n",
    "\n",
    "print(type(train_dataloader))\n",
    "print(iter(train_dataloader).next().keys())\n",
    "\n",
    "print(type(test_dataloader))\n",
    "print(iter(test_dataloader).next().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5696\n",
      "45408\n",
      "5696\n",
      "\n",
      "Sum\n",
      "56800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 32\n",
    "print(len(eval_dataloader)*batch)\n",
    "print(len(train_dataloader)*batch)\n",
    "print(len(test_dataloader)*batch)\n",
    "print()\n",
    "# sum\n",
    "print('Sum')\n",
    "print(len(eval_dataloader)*batch + len(train_dataloader)*batch + len(test_dataloader)*batch)\n",
    "\n",
    "\n",
    "# get batch size\n",
    "next(iter(eval_dataloader))['targets'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "electraModel = ElectraModel.from_pretrained('google/electra-small-discriminator')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "dataiter = iter(eval_dataloader)\n",
    "n_features = next(dataiter)['network_features'].shape[1]\n",
    "print(n_features)\n",
    "\n",
    "class ElectraClassifier(nn.Module):\n",
    "    def __init__(self,num_labels=2):\n",
    "        super(ElectraClassifier,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # network features\n",
    "        self.network_input = nn.Linear(in_features=9,out_features=2048) # 9 network features\n",
    "        self.dense_net2 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net3 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net4 = nn.Linear(in_features=2048,out_features=2048)\n",
    "\n",
    "        # output layer\n",
    "        self.out_proj = nn.Linear(2048, self.num_labels)\n",
    "\n",
    "\n",
    "    def forward(self,network_features=None):\n",
    "        x_net = self.network_input(network_features)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net2(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net3(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net4(x_net)\n",
    "        x = F.gelu(x_net)\n",
    "\n",
    "        logits = self.out_proj(x_net)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model=ElectraClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1563, 0.1171]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.5098, 0.4902]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "iter(test_dataloader).next()['targets']\n",
    "\n",
    "# random torch tensor\n",
    "network_features = torch.rand(1, 524)\n",
    "\n",
    "dense=nn.Linear(in_features=524,out_features=2)\n",
    "\n",
    "sm=nn.Softmax(dim=1)\n",
    "\n",
    "# torch.max(sm(dense(network_features)),dim=1)\n",
    "print(dense(network_features))\n",
    "print(sm(dense(network_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False,no_deprecation_warning=True)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "      network_features = d[\"network_features\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        network_features=network_features)\n",
    "\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  \n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    network_features = d[\"network_features\"].to(device)\n",
    "    \n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      network_features=network_features)\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = defaultdict(list)\n",
    "# best_accuracy = 0\n",
    "# for epoch in tqdm(range(EPOCHS)):\n",
    "\n",
    "#   train_acc, train_loss = train_epoch(model, train_dataloader, loss_fn, optimizer, device, scheduler,len(train_dataloader.dataset))\n",
    "#   val_acc, val_loss = eval_model(model, eval_dataloader, loss_fn, device, len(eval_dataloader.dataset))\n",
    "\n",
    "#   history['train_acc'].append(train_acc)\n",
    "#   history['train_loss'].append(train_loss)\n",
    "#   history['val_acc'].append(val_acc)\n",
    "#   history['val_loss'].append(val_loss)\n",
    "  \n",
    "#   if val_acc > best_accuracy:\n",
    "#     torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "#     best_accuracy = val_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "electraModel = ElectraModel.from_pretrained('google/electra-small-discriminator')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "dataiter = iter(eval_dataloader)\n",
    "n_features = next(dataiter)['network_features'].shape[1]\n",
    "print(n_features)\n",
    "\n",
    "class ElectraClassifier(nn.Module):\n",
    "    def __init__(self,num_labels=2):\n",
    "        super(ElectraClassifier,self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # network features\n",
    "        self.network_input = nn.Linear(in_features=9,out_features=2048) # 9 network features\n",
    "        self.dense_net2 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net3 = nn.Linear(in_features=2048,out_features=2048)\n",
    "        self.dense_net4 = nn.Linear(in_features=2048,out_features=2048)\n",
    "\n",
    "        # output layer\n",
    "        self.out_proj = nn.Linear(2048, 2)\n",
    "\n",
    "\n",
    "    def forward(self,network_features=None):\n",
    "        x_net = self.network_input(network_features)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net2(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net3(x_net)\n",
    "        x_net = F.gelu(x_net)\n",
    "        x_net = self.dense_net4(x_net)\n",
    "        x = F.gelu(x_net)\n",
    "\n",
    "        logits = self.out_proj(x_net)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "model=ElectraClassifier()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 9\n",
    "hidden_sizes = [2048, 64]\n",
    "output_size = 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.Softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([[0.9577, 0.0423],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.6927, 0.3073],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.8944, 0.1056],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.6693, 0.3307],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    d = next(iter(test_dataloader))\n",
    "    network_features = d[\"network_features\"].to(device)\n",
    "    logits = model(network_features)\n",
    "    _, preds = torch.max(logits, dim=1)\n",
    "    print(preds)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 0, 1, 1])\n",
      "tensor([[0.9577, 0.0423],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.6927, 0.3073],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.8944, 0.1056],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.4973, 0.5027],\n",
      "        [0.6693, 0.3307],\n",
      "        [0.0000, 1.0000],\n",
      "        [0.0000, 1.0000]])\n",
      "tensor([0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    ground_truth = []\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            network_features = d[\"network_features\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(network_features)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            ground_truth.extend(targets)\n",
    "            break\n",
    "\n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    ground_truth = torch.stack(ground_truth).cpu()\n",
    "\n",
    "    return predictions, prediction_probs, ground_truth\n",
    "\n",
    "y_pred, y_pred_probs, y_test = get_predictions(model,test_dataloader)\n",
    "\n",
    "print(y_pred)\n",
    "print(y_pred_probs)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5674"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_dataloader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1f814a4e109c882a9affb42993ee939858e43ebdc57bfb1498f07e0b82aab6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
